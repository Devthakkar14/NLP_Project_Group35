{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the LSTM model\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9742 - loss: 0.1831\n",
      "Epoch 1: val_accuracy improved from -inf to 0.98333, saving model to C:/Users/hp/Downloads/NLP Project/NLP_Project_Group35/baseline_best_model.keras\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 100ms/step - accuracy: 0.9742 - loss: 0.1828 - val_accuracy: 0.9833 - val_loss: 0.0831\n",
      "Epoch 2/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9795 - loss: 0.0975\n",
      "Epoch 2: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 88ms/step - accuracy: 0.9795 - loss: 0.0975 - val_accuracy: 0.9833 - val_loss: 0.0862\n",
      "Epoch 3/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9784 - loss: 0.0995\n",
      "Epoch 3: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 95ms/step - accuracy: 0.9784 - loss: 0.0995 - val_accuracy: 0.9833 - val_loss: 0.0826\n",
      "Epoch 4/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9826 - loss: 0.0751\n",
      "Epoch 4: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 107ms/step - accuracy: 0.9826 - loss: 0.0752 - val_accuracy: 0.9824 - val_loss: 0.0864\n",
      "Epoch 5/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9866 - loss: 0.0662\n",
      "Epoch 5: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 108ms/step - accuracy: 0.9866 - loss: 0.0662 - val_accuracy: 0.9833 - val_loss: 0.0847\n",
      "Epoch 6/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9874 - loss: 0.0658\n",
      "Epoch 6: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 115ms/step - accuracy: 0.9874 - loss: 0.0658 - val_accuracy: 0.9833 - val_loss: 0.0862\n",
      "Epoch 7/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9867 - loss: 0.0699\n",
      "Epoch 7: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 101ms/step - accuracy: 0.9867 - loss: 0.0698 - val_accuracy: 0.9824 - val_loss: 0.0941\n",
      "Epoch 8/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9908 - loss: 0.0514\n",
      "Epoch 8: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 106ms/step - accuracy: 0.9908 - loss: 0.0515 - val_accuracy: 0.9824 - val_loss: 0.0914\n",
      "Epoch 9/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9868 - loss: 0.0682\n",
      "Epoch 9: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 104ms/step - accuracy: 0.9868 - loss: 0.0681 - val_accuracy: 0.9833 - val_loss: 0.0949\n",
      "Epoch 10/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9863 - loss: 0.0711\n",
      "Epoch 10: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 104ms/step - accuracy: 0.9863 - loss: 0.0710 - val_accuracy: 0.9833 - val_loss: 0.0990\n",
      "Training the GRU model\n",
      "Epoch 1/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9511 - loss: 0.2194\n",
      "Epoch 1: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 135ms/step - accuracy: 0.9512 - loss: 0.2189 - val_accuracy: 0.9833 - val_loss: 0.0846\n",
      "Epoch 2/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9782 - loss: 0.1007\n",
      "Epoch 2: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 147ms/step - accuracy: 0.9782 - loss: 0.1007 - val_accuracy: 0.9833 - val_loss: 0.0837\n",
      "Epoch 3/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9822 - loss: 0.0779\n",
      "Epoch 3: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 129ms/step - accuracy: 0.9821 - loss: 0.0780 - val_accuracy: 0.9833 - val_loss: 0.0836\n",
      "Epoch 4/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9828 - loss: 0.0801\n",
      "Epoch 4: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 119ms/step - accuracy: 0.9828 - loss: 0.0800 - val_accuracy: 0.9769 - val_loss: 0.0958\n",
      "Epoch 5/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9883 - loss: 0.0582\n",
      "Epoch 5: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 116ms/step - accuracy: 0.9883 - loss: 0.0582 - val_accuracy: 0.9833 - val_loss: 0.0888\n",
      "Epoch 6/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.9889 - loss: 0.0601\n",
      "Epoch 6: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 140ms/step - accuracy: 0.9889 - loss: 0.0601 - val_accuracy: 0.9824 - val_loss: 0.1015\n",
      "Epoch 7/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9886 - loss: 0.0613\n",
      "Epoch 7: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 141ms/step - accuracy: 0.9886 - loss: 0.0613 - val_accuracy: 0.9833 - val_loss: 0.1058\n",
      "Epoch 8/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9891 - loss: 0.0562\n",
      "Epoch 8: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 169ms/step - accuracy: 0.9891 - loss: 0.0562 - val_accuracy: 0.9833 - val_loss: 0.0994\n",
      "Epoch 9/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9905 - loss: 0.0354\n",
      "Epoch 9: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 164ms/step - accuracy: 0.9905 - loss: 0.0354 - val_accuracy: 0.9796 - val_loss: 0.1115\n",
      "Epoch 10/10\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9908 - loss: 0.0273\n",
      "Epoch 10: val_accuracy did not improve from 0.98333\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 140ms/step - accuracy: 0.9908 - loss: 0.0273 - val_accuracy: 0.9722 - val_loss: 0.1280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23e876a6850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Function to create binary labels\n",
    "def create_labels(data, column, keyword):\n",
    "    return data[column].apply(lambda x: 1 if keyword.lower() in x.lower() else 0).values\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('C:/Users/hp/Downloads/NLP Project/NLP_Recipe_train.csv')\n",
    "test_data = pd.read_csv('C:/Users/hp/Downloads/NLP Project/NLP_Recipe_test.csv')\n",
    "\n",
    "# Create binary labels based on the presence of \"vegetarian\" in the description\n",
    "train_labels = create_labels(train_data, 'description', 'vegetarian')\n",
    "test_labels = create_labels(test_data, 'description', 'vegetarian')\n",
    "\n",
    "# Parameters for tokenization and padding\n",
    "vocab_size = 10000  # Size of the vocabulary\n",
    "max_length = 200    # Maximum length of each sequence\n",
    "trunc_type = 'post' # Truncate the sequences from the end\n",
    "padding_type = 'post' # Pad the sequences at the end\n",
    "oov_tok = \"<OOV>\"   # Token for out-of-vocabulary words\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_data['steps'])\n",
    "\n",
    "# Convert texts to sequences of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['steps'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['steps'])\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Define and compile the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(vocab_size, 64, input_length=max_length),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification\n",
    "])\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Define checkpoint callback to save the best model\n",
    "checkpoint_path = \"C:/Users/hp/Downloads/NLP Project/NLP_Project_Group35/baseline_best_model.keras\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "print(\"Training the LSTM model\")\n",
    "# Train the LSTM model\n",
    "lstm_model.fit(train_padded, train_labels, epochs=10, validation_data=(test_padded, test_labels), callbacks=[checkpoint])\n",
    "\n",
    "# Define and compile the GRU model\n",
    "gru_model = Sequential([\n",
    "    Embedding(vocab_size, 64, input_length=max_length),\n",
    "    GRU(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification\n",
    "])\n",
    "gru_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "print(\"Training the GRU model\")\n",
    "# Train the GRU model\n",
    "gru_model.fit(train_padded, train_labels, epochs=10, validation_data=(test_padded, test_labels), callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "BLEU Scores: [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "model = load_model(\"C:/Users/hp/Downloads/NLP Project/NLP_Project_Group35/baseline_best_model.keras\")\n",
    "\n",
    "# Function to generate text based on the model's prediction\n",
    "def generate_text(model, sequence):\n",
    "    # Predict the probability of the sequence being in the positive class\n",
    "    probability = model.predict(sequence)[0][0]\n",
    "    \n",
    "    # Generate a text output based on the probability threshold (e.g., 0.5)\n",
    "    if probability > 0.5:\n",
    "        return \"Vegetarian\"\n",
    "    else:\n",
    "        return \"Non-Vegetarian\"\n",
    "\n",
    "# Load and prepare your test data\n",
    "test_data = pd.read_csv('C:/Users/hp/Downloads/NLP Project/NLP_Recipe_test.csv')\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(test_data['steps'])  # Assuming 'steps' is the column for text\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['steps'])\n",
    "test_padded = pad_sequences(test_sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Generate texts for the first 5 test samples\n",
    "generated_texts = [generate_text(model, test_padded[i].reshape(1, -1)) for i in range(5)]\n",
    "\n",
    "# Define reference text (not meaningful in this context, used for demonstration)\n",
    "reference_texts = [[\"this\", \"is\", \"a\", \"reference\", \"text\"]]\n",
    "\n",
    "# Calculate BLEU scores for the first 5 generated texts (conceptually incorrect usage of BLEU)\n",
    "bleu_scores = [sentence_bleu([reference_texts[0]], generated_text.split()) for generated_text in generated_texts]\n",
    "print(\"BLEU Scores:\", bleu_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
